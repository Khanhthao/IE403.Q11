*Tổng quan: 
- Giới thiệu ViSoBERT, mô hình ngôn ngữ tiền huấn luyện đơn ngữ đầu tiên cho văn bản Việt trên mạng xã hội.
- Dựa trên kiến trúc XLM-R, mô hình trên được huấn luyện trên một tập dữ liệu quy mô lớn, chất lượng cao và đa dạng, thu thập từ các nền tảng phổ biến như Facebook, TikTok,...
- Đánh giá ViSoBERT trên 5 tác vụ NLP quan trọng: Nhận diện cảm xúc (Emotion Recognition), Phát hiện ngôn từ thù ghét (Hate Speech Detection), Phân tích cảm xúc (Sentiment Analysis), Phát hiện đánh giá Spam (Spam Reviews Detection) và Xác định phạm vi ngôn từ thù ghét (Hate Speech Spans Detection).
*Vấn đề:
- Có tồn tại các mô hình Tiếng Việt như PhoBERT, viBERT, vELECTRA nhưng những mô hình kể trên được huấn luyện trên Wikipedia, các tin tức
-> Nên không updata cho ngôn ngữ mạng xã hội (teencode, viết không dấu, emoji, slang,...)
-> Hiệu quả kém trong các tác vụ NLP.
*Nền tảng của các mô hình ngôn ngữ tiền huấn luyện cho các văn bản mạng 
- Mô hình pre-trained cho Tiếng Việt: 
+ PhoBERT
+ viBERT
+ vELECTRA
+ viBERT4news
+ BARTpho
+ ViT5
+ ViHealthBERT
--> Được huấn luyện trên văn bản chính thống (Wiki, báo chí, y tế), không chuyên mạng xã hội.
- Mô hình pre-trained cho văn bản mạng xã hội:
+ BERTweet
+ IndoBERTweet
+ RoBERTuito
+ TWiIBERT
+ Bernice
+ TwHIN-BERT
+ XLM-T
--> Có nhiều mô hình social media cho Twitter (Anh, Indo, Tây Ban Nha, đa ngôn ngữ), nhưng Twitter ít phổ biến ở Việt Nam. Vì thế, các mô hình này không phù hợp cho văn bản mạng xã hội tiếng Việt (nơi Facebook, TikTok, YouTube mới là chủ đạo).
--> Sự ra đời của ViSoBERT
* ViSoBERT:
- Nguồn dữ liệu: Facebook, Tiktok, Youtube.
- Cách thu thập: Dùng API chính thức của từng nền tảng (Graph API, TikTok Reasearch API, Youtube Data API).
- Tiền xử lý:
+ Xóa text không chuẩn (link, @username, spam vô nghĩa).
+ Giữ lại emoji để mô hình học cảm xúc.
- Kết quả: thu được ~1GB text chưa nén --> chỉ dùng cho mục đính nghiên cứu.
* Model Architecture (Kiến trúc mô hình):
- Dựa trên transformer - XLM-R.
- Cấu hình: 12 layers (encoder), 12 self-attention heads, hidden size = 768, ~97M tham số.
- Mục tiêu huấn luyện: Masked Language Modeling (MLM), giống BERT. 
- So sánh: nhẹ hơn PhoBERT Large (370M tham số) --> dễ triển khai hơn.
* Vietnamese Social Media Tokenizer (Tokenizer tùy chỉnh).
- Lần đầu tiên xây dựng tokenizer riêng cho Tiếng Việt mạng xã hội. 
- Công cụ: SentencePiece (không mất dữ liệu, xử lý tốt hơn Byte-Pair Encoding).
- Ưu điểm:
+ Biểu diễn ngắn gọn hơn (ít subword hơn).
+ Giữ nguyên emoji và teencode như một đơn vị có nghĩa --> giúp mô hình hiểu ngữ cảnh.
Ví dụ: Comment: d4y l4 vj du cko mot cau teencode
- ViSoBERT: ["d", "4", "y", "l", "4", "vj", "du", "cko", "mot", "cau", "teen", "code"]
- PhoBERT: ["d@@", "4@@", "y", "l@@", "4", "v@@", "j", "du", "ck@@", "o", "mo@@", "t", "ca@@", "u", "te@@", "en@@", "co@@", "de"].
--> ViSoBERT ít bị tách vụn, giữ đúng ngữ nghĩa của teencode.
* Thí nghiệm và Kết quả:
- Thiết lập thí nghiệm:
+ Huấn luyện (Downstream tasks).
+ Baseline models: Monolingual language models, multilingual language models và multilingual social media language models.
- Kết quả chính:
+ Nhiệm vụ nhận định cảm xúc (Emotion Recognition Task).
+ Nhiệm vụ nhận diện ngôn từ thù ghét (Hate Speech Detection Task).
+ Sentiment Analysis Task.
+ Spam Reviews Detection Task.
+ Hate Speech Spans Detection Task.
+ Multilingual social media PLMs.
* Kết quả thống kê và thảo luận.
* Kết luận và định hướng tương lai.
* Phần mở rộng:
- Demo ứng dụng thực tế.
- Thêm case study: ứng dụng vào phát hiện bình luận toxic trong các livestream.
- Phân tích ngôn ngữ đa phương thức (emoji, meme,...)
- Thêm so sánh giữa PhoBERT, TwHIN-BERT và ViSoBERT.
